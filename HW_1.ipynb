{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IvnGenza/Data-Science/blob/main/HW_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1!**"
      ],
      "metadata": {
        "id": "WbfjK8SgGbyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Given names mining***"
      ],
      "metadata": {
        "id": "wD4tszz6Gf5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "base_url = 'https://en.wikipedia.org/'            #first part of url for connecting with index of next pages\n",
        "\n",
        "url =  \"https://en.wikipedia.org/wiki/Category:Given_names\"\n",
        "\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "next_page = soup.findAll('a',string='next page')\n",
        "\n",
        "names = []\n",
        "\n",
        "\n",
        "while True:\n",
        "\n",
        "  h3 = soup.findAll('h3')[10:]\n",
        "\n",
        "  h3_name_flags = list(filter(lambda x: x.text.isupper() ,h3))\n",
        "  name_elements = list(map(lambda x:x.find_parent().findAll('a'),h3_name_flags))\n",
        "\n",
        "  ele_list=[]\n",
        "  for ele in name_elements:\n",
        "    ele_list.extend(ele)\n",
        "\n",
        "  name_elements = ele_list\n",
        "\n",
        "  plus_names = [i.text for i in name_elements]\n",
        "  names = names + plus_names\n",
        "\n",
        "  try:\n",
        "    href_value = next_page[-1].get('href')             #Getting index of a next page\n",
        "  except  IndexError:\n",
        "    break\n",
        "\n",
        "  next_url = base_url+href_value                    #url of next page\n",
        "\n",
        "  response = requests.get(next_url)\n",
        "  soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "  next_page = soup.findAll('a',string='next page')"
      ],
      "metadata": {
        "id": "FjRliDyX8LGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "result_tuples = []\n",
        "names = iter(names)\n",
        "while True:\n",
        "\n",
        "  try:\n",
        "    name = next(names)\n",
        "  except Exception as e:\n",
        "    break\n",
        "\n",
        "# Construct the API URL\n",
        "  api_url = f'https://en.wikipedia.org/w/api.php?action=query&titles={name}&prop=pageprops&format=json'\n",
        "\n",
        "# Make the API request and convert the response to JSON\n",
        "  response = requests.get(api_url).json()\n",
        "\n",
        "# Try to Extract the page ID from the JSON response\n",
        "  try:\n",
        "    page_id = str(next(iter(response['query']['pages'].values()))['pageprops']['wikibase_item'])\n",
        "\n",
        "  except Exception as e:\n",
        "    continue\n",
        "\n",
        "  url = f\"https://www.wikidata.org/wiki/{page_id}\"\n",
        "  response1 = requests.get(url)\n",
        "  soup = BeautifulSoup(response1.text, \"html.parser\")\n",
        "  my_blocks = soup.find('ul',class_='wikibase-sitelinklistview-listview').findAll(recursive=False)\n",
        "\n",
        "  for my_block in my_blocks:\n",
        "    s1=my_block.findAll({'span':True})\n",
        "\n",
        "    my_description = soup.find('span',class_ = 'wikibase-descriptionview-text').text\n",
        "    my_language = s1[1].get('title')\n",
        "    my_translate = s1[2].text\n",
        "    my_lang = s1[3].get('lang')\n",
        "\n",
        "    result = (name,page_id,my_description,my_language,my_lang,my_translate)\n",
        "    result_tuples.append(result)\n",
        "\n",
        "\n",
        "\n",
        "results_df = pd.DataFrame(result_tuples, columns=['Label','WikiData ID','English Description','Language','Wiki Short Lang','Entry'])\n",
        "results_df.to_csv('names.csv', index=False, encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "-AgPO7mlUIsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Part 2!***"
      ],
      "metadata": {
        "id": "KC1wLOT1yWEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Surnames mining:***"
      ],
      "metadata": {
        "id": "nSkJ4W0D-PUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "base_url = 'https://en.wikipedia.org/'            #first part of url for connecting with index of next pages\n",
        "\n",
        "url =  \"https://en.wikipedia.org/wiki/Category:Surnames\"\n",
        "\n",
        "session = requests.Session()\n",
        "\n",
        "response = session.get(url)\n",
        "\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "next_page = soup.findAll('a',string='next page')\n",
        "\n",
        "names = []\n",
        "\n",
        "\n",
        "while True:\n",
        "\n",
        "  h3 = soup.findAll('h3')[13:]\n",
        "  h3_name_flags = list(filter(lambda x: x.text.isupper() ,h3))\n",
        "  name_elements = list(map(lambda x:x.find_parent().findAll('a'),tqdm(h3_name_flags)))\n",
        "\n",
        "  ele_list=[]\n",
        "  for ele in name_elements:\n",
        "    ele_list.extend(ele)\n",
        "\n",
        "  name_elements = ele_list\n",
        "\n",
        "  plus_names = [i.text for i in name_elements]\n",
        "  names = names + plus_names\n",
        "\n",
        "  try:\n",
        "    href_value = next_page[-1].get('href')             #Getting index of a next page\n",
        "\n",
        "  except  IndexError:\n",
        "    break\n",
        "\n",
        "  next_url = base_url+href_value                    #url of next page\n",
        "\n",
        "  response = session.get(next_url)\n",
        "  soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "  next_page = soup.findAll('a',string='next page')"
      ],
      "metadata": {
        "id": "T6LePpiMgJK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.exceptions import ConnectionError\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import multiprocessing\n",
        "from tqdm import tqdm\n",
        "\n",
        "#########################################################################################################\n",
        "#########################################################################################################\n",
        "wiki_adapter = HTTPAdapter(max_retries=99)\n",
        "session1 = requests.Session()\n",
        "session1.mount('https://en.wikipedia.org',wiki_adapter)\n",
        "\n",
        "\n",
        "def get_id(name):         # func for catching ids\n",
        "  # Construct the API URL\n",
        "  api_url = f'https://en.wikipedia.org/w/api.php?action=query&titles={name}&prop=pageprops&format=json'\n",
        "\n",
        "# Make the API request and convert the response to JSON\n",
        "  try:\n",
        "    response = session1.get(api_url).json()\n",
        "  except ConnectionError as ce:\n",
        "    print(ce)\n",
        "\n",
        "# Try to Extract the page ID from the JSON response\n",
        "  try:\n",
        "    page_id = str(next(iter(response['query']['pages'].values()))['pageprops']['wikibase_item'])\n",
        "    #print(page_id)\n",
        "    return page_id\n",
        "\n",
        "  except Exception as e:\n",
        "    return None\n",
        "\n",
        "#########################################################################################################\n",
        "#########################################################################################################\n",
        "\n",
        "wdata_adapter = HTTPAdapter(max_retries=99)\n",
        "session2 = requests.Session()\n",
        "session2.mount('https://www.wikidata.org',wdata_adapter)\n",
        "session2.headers.update({'User-Agent': 'Custom user agent'})\n",
        "\n",
        "def get_name_info(page_id):\n",
        "    url = f\"https://www.wikidata.org/wiki/{page_id}\"\n",
        "\n",
        "    try:\n",
        "      response1 = session2.get(url)\n",
        "      trash = response1.content\n",
        "    except ConnectionError as er:\n",
        "      print('>>>',er)\n",
        "\n",
        "\n",
        "    soup = BeautifulSoup(response1.text, \"html.parser\")\n",
        "    my_blocks = soup.find('ul',class_='wikibase-sitelinklistview-listview').findAll(recursive=False)    #all elements we need from some name`s page.\n",
        "\n",
        "    my_name = soup.find('span',class_ = 'wikibase-title-label').text                                    #Element with a main name`s translate\n",
        "    my_description = soup.find('span',class_ = 'wikibase-descriptionview-text').text                    #Element with description of a current name\n",
        "\n",
        "    for my_block in my_blocks:\n",
        "        s1=my_block.findAll({'span':True})                  #All elements we need of every additional translate name\n",
        "\n",
        "        my_language = s1[1].get('title')\n",
        "        my_translate = s1[2].text\n",
        "        my_lang = s1[3].get('lang')\n",
        "\n",
        "        result = (my_name,page_id,my_description,my_language,my_lang,my_translate)\n",
        "        result_tuples.append(result)    # appending tuple to a manager(list)\n",
        "\n",
        "#########################################################################################################\n",
        "#########################################################################################################\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    manager = multiprocessing.Manager()\n",
        "    result_tuples = manager.list()      # Manager for collecting all tuples from multiprocessing in one data structure.\n",
        "\n",
        "    with multiprocessing.Pool(4) as pool: # Multiprocessing\n",
        "\n",
        "      all_ids = pool.map(get_id ,tqdm(names))      # Mapping all names we found with get_id func in order to find and save all IDs.\n",
        "      valid_ids = filter(lambda x:x is not None, tqdm(all_ids)) # Removing elements with value \"None\".\n",
        "      pool.map(get_name_info, tqdm(list(valid_ids)))\n",
        "\n",
        "    result_tuples = list(result_tuples) #Convert manager object to\n",
        "    results_df = pd.DataFrame(result_tuples, columns=['Label','WikiData ID','English Description','Language','Wiki Short Lang','Entry'])\n",
        "    results_df.to_csv('family_names.csv', index=False, encoding='utf-8-sig')\n"
      ],
      "metadata": {
        "id": "U4r9qLZJWyHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7d43e8b-43b7-4cf7-b8c7-868ed8abc49c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94411/94411 [19:18<00:00, 81.47it/s]\n",
            "100%|██████████| 94411/94411 [00:00<00:00, 2091768.10it/s]\n",
            "100%|██████████| 70646/70646 [1:37:12<00:00, 12.11it/s]\n"
          ]
        }
      ]
    }
  ]
}